name: Build, Scan, and Deploy with IAM and Helm

on:
  push:
    branches: [ main ]

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: my-cluster
  ECR_REPOSITORY: kubernetes
  COMBINED_FILE: app1.yaml
  NAMESPACE: apps

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Run Trivy filesystem scan (SAST)
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        format: 'table'
        exit-code: '1'  # Changed to 1 to fail on critical vulnerabilities
        severity: 'CRITICAL'

  build:
    needs: test
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Login to ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v1

    - name: Build and push App1 image
      run: |
        docker build -t app1 -f Dockerfile .
        docker tag app1 ${{ steps.login-ecr.outputs.registry }}/kubernetes:${{ github.sha }}
        docker push ${{ steps.login-ecr.outputs.registry }}/kubernetes:${{ github.sha }}
        echo "APP1_IMAGE=${{ steps.login-ecr.outputs.registry }}/kubernetes:${{ github.sha }}" >> $GITHUB_ENV
     
  deploy:
    needs: build
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      with:
        fetch-depth: 0

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup infrastructure
      run: |
        chmod +x ./install.sh
        ./install.sh ${{ env.CLUSTER_NAME }} ${{ env.AWS_REGION }}

    - name: Deploy applications
      run: |
        # Create namespace
        kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -
        
        # Update image in manifest
        sed -i "s|ECR_IMAGE_PLACEHOLDER|${{ env.APP1_IMAGE }}|g" app1.yaml
        
        # Apply resources
        kubectl apply -f app1.yaml -n ${{ env.NAMESPACE }}
        
        # Verify deployments
        kubectl rollout status deployment/app1 -n ${{ env.NAMESPACE }} --timeout=300s
        kubectl rollout status deployment/app2 -n ${{ env.NAMESPACE }} --timeout=300s
        
        # Wait for ALB provision (up to 5 minutes)
        echo "Waiting for ALB to be provisioned..."
        for i in {1..30}; do
          INGRESS_HOST=$(kubectl get ingress -n ${{ env.NAMESPACE }} -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}')
          [ -n "$INGRESS_HOST" ] && break
          sleep 10
          echo "Attempt $i/30: ALB not yet provisioned..."
        done
        
        if [ -z "$INGRESS_HOST" ]; then
          echo "::error::ALB not provisioned after 5 minutes"
          echo "Debugging information:"
          kubectl get ingress -n ${{ env.NAMESPACE }} -o yaml
          kubectl describe ingress -n ${{ env.NAMESPACE }}
          kubectl logs -n kube-system deployment/aws-load-balancer-controller
          exit 1
        fi
        
        echo "ALB successfully provisioned:"
        echo "http://$INGRESS_HOST"